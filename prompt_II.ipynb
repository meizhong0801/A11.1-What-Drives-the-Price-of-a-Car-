{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What drives the price of a car?\n",
    "\n",
    "![](images/kurt.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OVERVIEW**\n",
    "\n",
    "In this application, you will explore a dataset from Kaggle. The original dataset contained information on 3 million used cars. The provided dataset contains information on 426K cars to ensure speed of processing.  Your goal is to understand what factors make a car more or less expensive.  As a result of your analysis, you should provide clear recommendations to your client -- a used car dealership -- as to what consumers value in a used car."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRISP-DM Framework\n",
    "\n",
    "<center>\n",
    "    <img src = images/crisp.png width = 50%/>\n",
    "</center>\n",
    "\n",
    "\n",
    "To frame the task, throughout our practical applications, we will refer back to a standard process in industry for data projects called CRISP-DM.  This process provides a framework for working through a data problem.  Your first step in this application will be to read through a brief overview of CRISP-DM [here](https://mo-pcco.s3.us-east-1.amazonaws.com/BH-PCMLAI/module_11/readings_starter.zip).  After reading the overview, answer the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Understanding\n",
    "\n",
    "From a business perspective, we are tasked with identifying key drivers for used car prices.  In the CRISP-DM overview, we are asked to convert this business framing to a data problem definition.  Using a few sentences, reframe the task as a data task with the appropriate technical vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Our goal is to analyze data on used cars to understand what factors affect their prices.<br>We will treat price as the target variable and look at other details like car age, mileage, brand, condition, and fuel type as predictor variables <br>By using data analysis and modeling, we will find patterns that show which features make a car more valuable. This will help dealerships set better prices and choose the right cars for their inventory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding\n",
    "\n",
    "After considering the business understanding, we want to get familiar with our data.  Write down some steps that you would take to get to know the dataset and identify any quality issues within.  Take time to get to know the dataset and explore what information it contains and how this could be used to inform your business understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here are my steps for data understanding \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load and inspect the data, first read the dataset and check the first few rows, identify the features of the columns and their data types, and count the number of missing values ​​in each column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Identify key variables such as price, year, odometer, manufacturer, condition, and then assess how many records have missing or incomplete data. Decide whether to fill, delete, or ignore missing values ​​based on their importance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Detect outliers, such as unrealistic prices such as $0, extremely high values. Also, whether the odometer reading has unusually low or high mileage. Finally, check the year to ensure that there are no incorrect values, such as future years, very old cars.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Understand the feature distribution and relationship, draw histograms and box plots for the numerical variables price and odometer, and then use scatter plots to examine the relationship between car age, mileage, and price.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Assess data consistency and ensure that all records have valid state and region values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Prepare business understanding insights to determine which features have the greatest impact on price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "After our initial exploration and fine-tuning of the business understanding, it is time to construct our final dataset prior to modeling.  Here, we want to make sure to handle any integrity issues and cleaning, the engineering of new features, any transformations that we believe should happen (scaling, logarithms, normalization, etc.), and general preparation for modeling with `sklearn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 426880 entries, 0 to 426879\n",
      "Data columns (total 18 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   id            426880 non-null  int64  \n",
      " 1   region        426880 non-null  object \n",
      " 2   price         426880 non-null  int64  \n",
      " 3   year          425675 non-null  float64\n",
      " 4   manufacturer  409234 non-null  object \n",
      " 5   model         421603 non-null  object \n",
      " 6   condition     252776 non-null  object \n",
      " 7   cylinders     249202 non-null  object \n",
      " 8   fuel          423867 non-null  object \n",
      " 9   odometer      422480 non-null  float64\n",
      " 10  title_status  418638 non-null  object \n",
      " 11  transmission  424324 non-null  object \n",
      " 12  VIN           265838 non-null  object \n",
      " 13  drive         296313 non-null  object \n",
      " 14  size          120519 non-null  object \n",
      " 15  type          334022 non-null  object \n",
      " 16  paint_color   296677 non-null  object \n",
      " 17  state         426880 non-null  object \n",
      "dtypes: float64(2), int64(2), object(14)\n",
      "memory usage: 58.6+ MB\n",
      "None\n",
      "           id                  region  price  year manufacturer model  \\\n",
      "0  7222695916                prescott   6000   NaN          NaN   NaN   \n",
      "1  7218891961            fayetteville  11900   NaN          NaN   NaN   \n",
      "2  7221797935            florida keys  21000   NaN          NaN   NaN   \n",
      "3  7222270760  worcester / central MA   1500   NaN          NaN   NaN   \n",
      "4  7210384030              greensboro   4900   NaN          NaN   NaN   \n",
      "\n",
      "  condition cylinders fuel  odometer title_status transmission  VIN drive  \\\n",
      "0       NaN       NaN  NaN       NaN          NaN          NaN  NaN   NaN   \n",
      "1       NaN       NaN  NaN       NaN          NaN          NaN  NaN   NaN   \n",
      "2       NaN       NaN  NaN       NaN          NaN          NaN  NaN   NaN   \n",
      "3       NaN       NaN  NaN       NaN          NaN          NaN  NaN   NaN   \n",
      "4       NaN       NaN  NaN       NaN          NaN          NaN  NaN   NaN   \n",
      "\n",
      "  size type paint_color state  \n",
      "0  NaN  NaN         NaN    az  \n",
      "1  NaN  NaN         NaN    ar  \n",
      "2  NaN  NaN         NaN    fl  \n",
      "3  NaN  NaN         NaN    ma  \n",
      "4  NaN  NaN         NaN    nc  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = r\"C:\\Users\\user\\Downloads\\practical_application_II_starter\\data\\vehicles.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "print(df.info())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned dataset saved as 'vehicles_cleaned.csv'.\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = ['manufacturer', 'model', 'condition', 'cylinders', 'fuel', \n",
    "                       'title_status', 'transmission', 'drive', 'size', 'type', 'paint_color']\n",
    "df[categorical_columns] = df[categorical_columns].fillna(\"Unknown\")\n",
    "df['year'].fillna(df['year'].median(), inplace=True)\n",
    "df['odometer'].fillna(df['odometer'].median(), inplace=True)\n",
    "df = df[(df['price'] > 500) & (df['price'] < 100000)]\n",
    "df.to_csv(\"vehicles_cleaned.csv\", index=False)\n",
    "print(\"\\nCleaned dataset saved as 'vehicles_cleaned.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 383068 entries, 0 to 383067\n",
      "Data columns (total 18 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   id            383068 non-null  int64  \n",
      " 1   region        383068 non-null  object \n",
      " 2   price         383068 non-null  int64  \n",
      " 3   year          383068 non-null  float64\n",
      " 4   manufacturer  383068 non-null  object \n",
      " 5   model         383068 non-null  object \n",
      " 6   condition     383068 non-null  object \n",
      " 7   cylinders     383068 non-null  object \n",
      " 8   fuel          383068 non-null  object \n",
      " 9   odometer      383068 non-null  float64\n",
      " 10  title_status  383068 non-null  object \n",
      " 11  transmission  383068 non-null  object \n",
      " 12  VIN           235330 non-null  object \n",
      " 13  drive         383068 non-null  object \n",
      " 14  size          383068 non-null  object \n",
      " 15  type          383068 non-null  object \n",
      " 16  paint_color   383068 non-null  object \n",
      " 17  state         383068 non-null  object \n",
      "dtypes: float64(2), int64(2), object(14)\n",
      "memory usage: 52.6+ MB\n",
      "None\n",
      "           id                  region  price    year manufacturer    model  \\\n",
      "0  7222695916                prescott   6000  2013.0      Unknown  Unknown   \n",
      "1  7218891961            fayetteville  11900  2013.0      Unknown  Unknown   \n",
      "2  7221797935            florida keys  21000  2013.0      Unknown  Unknown   \n",
      "3  7222270760  worcester / central MA   1500  2013.0      Unknown  Unknown   \n",
      "4  7210384030              greensboro   4900  2013.0      Unknown  Unknown   \n",
      "\n",
      "  condition cylinders     fuel  odometer title_status transmission  VIN  \\\n",
      "0   Unknown   Unknown  Unknown   85548.0      Unknown      Unknown  NaN   \n",
      "1   Unknown   Unknown  Unknown   85548.0      Unknown      Unknown  NaN   \n",
      "2   Unknown   Unknown  Unknown   85548.0      Unknown      Unknown  NaN   \n",
      "3   Unknown   Unknown  Unknown   85548.0      Unknown      Unknown  NaN   \n",
      "4   Unknown   Unknown  Unknown   85548.0      Unknown      Unknown  NaN   \n",
      "\n",
      "     drive     size     type paint_color state  \n",
      "0  Unknown  Unknown  Unknown     Unknown    az  \n",
      "1  Unknown  Unknown  Unknown     Unknown    ar  \n",
      "2  Unknown  Unknown  Unknown     Unknown    fl  \n",
      "3  Unknown  Unknown  Unknown     Unknown    ma  \n",
      "4  Unknown  Unknown  Unknown     Unknown    nc  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "file_path = r\"C:\\Users\\user\\Downloads\\practical_application_II_starter\\cleaned\\vehicles_cleaned.csv\"  \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(df.info())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   car_age  odometer odometer_category\n",
      "0     11.0   85548.0            Medium\n",
      "1     11.0   85548.0            Medium\n",
      "2     11.0   85548.0            Medium\n",
      "3     11.0   85548.0            Medium\n",
      "4     11.0   85548.0            Medium\n"
     ]
    }
   ],
   "source": [
    "df['car_age'] = 2024 - df['year']\n",
    "\n",
    "df['odometer_category'] = pd.cut(\n",
    "    df['odometer'], \n",
    "    bins=[0, 50000, 100000, np.inf], \n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "\n",
    "df.drop(columns=['year'], inplace=True)\n",
    "print(df[['car_age', 'odometer', 'odometer_category']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id                  region  price  manufacturer  model  condition  \\\n",
      "0  7222695916                prescott   6000             0   6642          0   \n",
      "1  7218891961            fayetteville  11900             0   6642          0   \n",
      "2  7221797935            florida keys  21000             0   6642          0   \n",
      "3  7222270760  worcester / central MA   1500             0   6642          0   \n",
      "4  7210384030              greensboro   4900             0   6642          0   \n",
      "\n",
      "  cylinders  odometer title_status  VIN  ... fuel_hybrid fuel_other  \\\n",
      "0   Unknown   85548.0      Unknown  NaN  ...           0          0   \n",
      "1   Unknown   85548.0      Unknown  NaN  ...           0          0   \n",
      "2   Unknown   85548.0      Unknown  NaN  ...           0          0   \n",
      "3   Unknown   85548.0      Unknown  NaN  ...           0          0   \n",
      "4   Unknown   85548.0      Unknown  NaN  ...           0          0   \n",
      "\n",
      "  transmission_automatic transmission_manual  transmission_other  \\\n",
      "0                      0                   0                   0   \n",
      "1                      0                   0                   0   \n",
      "2                      0                   0                   0   \n",
      "3                      0                   0                   0   \n",
      "4                      0                   0                   0   \n",
      "\n",
      "   drive_Unknown  drive_fwd  drive_rwd  odometer_category_Medium  \\\n",
      "0              1          0          0                         1   \n",
      "1              1          0          0                         1   \n",
      "2              1          0          0                         1   \n",
      "3              1          0          0                         1   \n",
      "4              1          0          0                         1   \n",
      "\n",
      "   odometer_category_High  \n",
      "0                       0  \n",
      "1                       0  \n",
      "2                       0  \n",
      "3                       0  \n",
      "4                       0  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "one_hot_cols = ['fuel', 'transmission', 'drive', 'odometer_category']\n",
    "label_cols = ['manufacturer', 'model', 'condition']\n",
    "\n",
    "df = pd.get_dummies(df, columns=one_hot_cols, drop_first=True)\n",
    "\n",
    "label_encoders = {}\n",
    "for col in label_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])  \n",
    "    label_encoders[col] = le  \n",
    "\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id                  region     price  manufacturer  model  \\\n",
      "0  7222695916                prescott -0.947981             0   6642   \n",
      "1  7218891961            fayetteville -0.174938             0   6642   \n",
      "2  7221797935            florida keys  0.466292             0   6642   \n",
      "3  7222270760  worcester / central MA -2.512585             0   6642   \n",
      "4  7210384030              greensboro -1.176594             0   6642   \n",
      "\n",
      "   condition cylinders  odometer title_status  VIN  ... fuel_hybrid  \\\n",
      "0          0   Unknown  -0.06814      Unknown  NaN  ...           0   \n",
      "1          0   Unknown  -0.06814      Unknown  NaN  ...           0   \n",
      "2          0   Unknown  -0.06814      Unknown  NaN  ...           0   \n",
      "3          0   Unknown  -0.06814      Unknown  NaN  ...           0   \n",
      "4          0   Unknown  -0.06814      Unknown  NaN  ...           0   \n",
      "\n",
      "  fuel_other transmission_automatic transmission_manual  transmission_other  \\\n",
      "0          0                      0                   0                   0   \n",
      "1          0                      0                   0                   0   \n",
      "2          0                      0                   0                   0   \n",
      "3          0                      0                   0                   0   \n",
      "4          0                      0                   0                   0   \n",
      "\n",
      "   drive_Unknown  drive_fwd  drive_rwd  odometer_category_Medium  \\\n",
      "0              1          0          0                         1   \n",
      "1              1          0          0                         1   \n",
      "2              1          0          0                         1   \n",
      "3              1          0          0                         1   \n",
      "4              1          0          0                         1   \n",
      "\n",
      "   odometer_category_High  \n",
      "0                       0  \n",
      "1                       0  \n",
      "2                       0  \n",
      "3                       0  \n",
      "4                       0  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "num_features = ['car_age', 'odometer', 'price']\n",
    "df['price'] = np.log1p(df['price'])\n",
    "scaler = StandardScaler()\n",
    "df[num_features] = scaler.fit_transform(df[num_features])\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (306454, 27), (306454,)\n",
      "Testing set: (76614, 27), (76614,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=['price'])  \n",
    "y = df['price']  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Testing set: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (306454, 27), (306454,)\n",
      "Testing set: (76614, 27), (76614,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training set: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Testing set: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot Encoding applied successfully to limited features!\n",
      "Updated feature set: (383068, 65)\n"
     ]
    }
   ],
   "source": [
    "# Identify categorical columns in the dataset\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Define high-cardinality columns that should not be One-Hot Encoded\n",
    "high_cardinality_cols = ['region', 'VIN', 'state']  # Too many unique values\n",
    "\n",
    "# Keep only categorical columns that are not in high-cardinality list\n",
    "categorical_cols = [col for col in categorical_cols if col not in high_cardinality_cols]\n",
    "\n",
    "# Apply One-Hot Encoding to the selected categorical columns\n",
    "X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "print(\"One-Hot Encoding applied successfully to limited features!\")\n",
    "print(\"Updated feature set:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot Encoding applied successfully to limited features!\n",
      "Updated feature set: (383068, 65)\n"
     ]
    }
   ],
   "source": [
    "high_cardinality_cols = ['region', 'VIN', 'state']  # Columns with too many unique values\n",
    "categorical_cols = [col for col in categorical_cols if col not in high_cardinality_cols]\n",
    "\n",
    "X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "print(\"One-Hot Encoding applied successfully to limited features!\")\n",
    "print(\"Updated feature set:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encoding applied to high-cardinality columns!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_cols = ['region', 'VIN', 'state']  # High-cardinality columns\n",
    "\n",
    "for col in label_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col])  # Convert to numeric values\n",
    "\n",
    "print(\"Label Encoding applied to high-cardinality columns!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot Encoding applied with category limits!\n"
     ]
    }
   ],
   "source": [
    "top_regions = X['region'].value_counts().index[:10]  # Keep top 10 regions\n",
    "X['region'] = X['region'].apply(lambda x: x if x in top_regions else 'Other')\n",
    "\n",
    "# Now apply One-Hot Encoding safely\n",
    "X = pd.get_dummies(X, columns=['region'], drop_first=True)\n",
    "\n",
    "print(\"One-Hot Encoding applied with category limits!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models trained and predictions made successfully!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Initialize models\n",
    "lr_model = LinearRegression()\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# Train models\n",
    "lr_model.fit(X_train, y_train)\n",
    "rf_model.fit(X_train, y_train)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "print(\"Models trained and predictions made successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Model: Linear Regression\n",
      "   MAE: 0.4619\n",
      "   RMSE: 0.6774\n",
      "   R² Score: 0.5446\n",
      "----------------------------------------\n",
      "🔹 Model: Random Forest Regressor\n",
      "   MAE: 0.1879\n",
      "   RMSE: 0.3814\n",
      "   R² Score: 0.8556\n",
      "----------------------------------------\n",
      "🔹 Model: XGBoost Regressor\n",
      "   MAE: 0.3010\n",
      "   RMSE: 0.4923\n",
      "   R² Score: 0.7595\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"🔹 Model: {model_name}\")\n",
    "    print(f\"   MAE: {mae:.4f}\")\n",
    "    print(f\"   RMSE: {rmse:.4f}\")\n",
    "    print(f\"   R² Score: {r2:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate each model\n",
    "evaluate_model(y_test, y_pred_lr, \"Linear Regression\")\n",
    "evaluate_model(y_test, y_pred_rf, \"Random Forest Regressor\")\n",
    "evaluate_model(y_test, y_pred_xgb, \"XGBoost Regressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with non-numeric data in X_train:\n",
      "Index(['region', 'cylinders', 'title_status', 'VIN', 'size', 'type',\n",
      "       'paint_color', 'state'],\n",
      "      dtype='object')\n",
      "Columns with non-numeric data in X_test:\n",
      "Index(['region', 'cylinders', 'title_status', 'VIN', 'size', 'type',\n",
      "       'paint_color', 'state'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns with non-numeric data in X_train:\")\n",
    "print(X_train.select_dtypes(include=['object']).columns)\n",
    "\n",
    "print(\"Columns with non-numeric data in X_test:\")\n",
    "print(X_test.select_dtypes(include=['object']).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All categorical columns have been encoded!\n",
      "Updated feature set: (306454, 65)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Apply Label Encoding\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "\n",
    "# Ensure train-test split is updated with numerical values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"All categorical columns have been encoded!\")\n",
    "print(\"Updated feature set:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26788\\1017877774.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m                            scoring='neg_mean_absolute_error', cv=3, verbose=2, n_jobs=-1)\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Best Parameters Found:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    889\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1390\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1391\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1392\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    836\u001b[0m                     )\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 838\u001b[1;33m                 out = parallel(\n\u001b[0m\u001b[0;32m    839\u001b[0m                     delayed(_fit_and_score)(\n\u001b[0;32m    840\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1056\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1057\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    933\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 935\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    936\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    937\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    439\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid,\n",
    "                           scoring='neg_mean_absolute_error', cv=3, verbose=2, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best Parameters Found:\", grid_search.best_params_)\n",
    "\n",
    "\n",
    "best_rf_model = RandomForestRegressor(**grid_search.best_params_, random_state=42)\n",
    "best_rf_model.fit(X_train, y_train)\n",
    "y_pred_best_rf = best_rf_model.predict(X_test)\n",
    "evaluate_model(y_test, y_pred_best_rf, \"Tuned Random Forest Regressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "With your (almost?) final dataset in hand, it is now time to build some models.  Here, you should build a number of different regression models with the price as the target.  In building your models, you should explore different parameters and be sure to cross-validate your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split completed successfully!\n",
      "Training set: (306454, 27), (306454,)\n",
      "Testing set: (76614, 27), (76614,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "X = df.drop(columns=['price'])  # Exclude price column (target)\n",
    "y = df['price']  # Price is the target variable\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Data split completed successfully!\")\n",
    "print(f\"Training set: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Testing set: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Non-numeric columns found: ['region', 'cylinders', 'title_status', 'VIN', 'size', 'type', 'paint_color', 'state']\n"
     ]
    }
   ],
   "source": [
    "# Check for object (categorical) columns\n",
    "non_numeric_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "print(\" Non-numeric columns found:\", non_numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = ['cylinders', 'title_status', 'size', 'type', 'paint_color']\n",
    "X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Label Encoding applied to high-cardinality features!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "high_cardinality_cols = ['region', 'VIN', 'state']  # Columns with too many unique values\n",
    "le = LabelEncoder()\n",
    "\n",
    "for col in high_cardinality_cols:\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "\n",
    "print(\" Label Encoding applied to high-cardinality features!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data ready for modeling!\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\" Data ready for modeling!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining non-numeric columns: []\n"
     ]
    }
   ],
   "source": [
    "print(\"Remaining non-numeric columns:\", X_train.select_dtypes(include=['object']).columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Linear Regression - Cross-Validated MAE: 0.4632\n",
      "🔹 Random Forest Regressor - Cross-Validated MAE: 0.2034\n",
      "🔹 XGBoost Regressor - Cross-Validated MAE: 0.3032\n",
      "\n",
      " Model Performance Summary:\n",
      " Random Forest Regressor: 0.2034 MAE\n",
      " XGBoost Regressor: 0.3032 MAE\n",
      " Linear Regression: 0.4632 MAE\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"XGBoost Regressor\": XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "}\n",
    "\n",
    "# Function to evaluate models using cross-validation\n",
    "def cross_validate_model(model, X_train, y_train):\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\n",
    "    return np.mean(-scores)  # Convert negative MAE to positive\n",
    "\n",
    "# Train & Evaluate Models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    mae = cross_validate_model(model, X_train, y_train)\n",
    "    results[name] = mae\n",
    "    print(f\"🔹 {name} - Cross-Validated MAE: {mae:.4f}\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\n Model Performance Summary:\")\n",
    "for model, score in sorted(results.items(), key=lambda x: x[1]):\n",
    "    print(f\" {model}: {score:.4f} MAE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "With some modeling accomplished, we aim to reflect on what we identify as a high-quality model and what we are able to learn from this.  We should review our business objective and explore how well we can provide meaningful insight into drivers of used car prices.  Your goal now is to distill your findings and determine whether the earlier phases need revisitation and adjustment or if you have information of value to bring back to your client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Models initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Initialize models\n",
    "lr_model = LinearRegression()\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "print(\" Models initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Models trained successfully!\n"
     ]
    }
   ],
   "source": [
    "# Train models\n",
    "lr_model.fit(X_train, y_train)\n",
    "rf_model.fit(X_train, y_train)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "print(\" Models trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions generated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "print(\"Predictions generated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Model: Linear Regression\n",
      "   MAE: 0.4622\n",
      "   RMSE: 0.6780\n",
      "   R² Score: 0.5438\n",
      "----------------------------------------\n",
      "🔹 Model: Random Forest Regressor\n",
      "   MAE: 0.1917\n",
      "   RMSE: 0.3822\n",
      "   R² Score: 0.8550\n",
      "----------------------------------------\n",
      "🔹 Model: XGBoost Regressor\n",
      "   MAE: 0.3032\n",
      "   RMSE: 0.4956\n",
      "   R² Score: 0.7562\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Function to evaluate models\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"🔹 Model: {model_name}\")\n",
    "    print(f\"   MAE: {mae:.4f}\")\n",
    "    print(f\"   RMSE: {rmse:.4f}\")\n",
    "    print(f\"   R² Score: {r2:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate all models\n",
    "evaluate_model(y_test, y_pred_lr, \"Linear Regression\")\n",
    "evaluate_model(y_test, y_pred_rf, \"Random Forest Regressor\")\n",
    "evaluate_model(y_test, y_pred_xgb, \"XGBoost Regressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is best_rf_model defined? False\n"
     ]
    }
   ],
   "source": [
    "print(\"Is best_rf_model defined?\", \"best_rf_model\" in globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize model\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, \n",
    "                           scoring='neg_mean_absolute_error', cv=3, verbose=2, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Assign the best model\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"Best Random Forest Model trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the saved model (adjust the filename if needed)\n",
    "best_rf_model = joblib.load(\"best_rf_model.pkl\")\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment\n",
    "\n",
    "Now that we've settled on our models and findings, it is time to deliver the information to the client.  You should organize your work as a basic report that details your primary findings.  Keep in mind that your audience is a group of used car dealers interested in fine-tuning their inventory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
